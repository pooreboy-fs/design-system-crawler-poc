name: Crawl & Deploy Static Site

on:
  # â”€â”€ Manual trigger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  workflow_dispatch:
    inputs:
      site_url:
        description: "URL of the site to crawl"
        required: true
        type: string
      max_pages:
        description: "Max pages to crawl"
        required: false
        default: "50"
        type: string
      wait_for_network:
        description: "Wait for full network idle (slower, more complete)"
        required: false
        default: "true"
        type: choice
        options:
          - "true"
          - "false"

  # â”€â”€ Schedule â€” uncomment + set your cron â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # schedule:
  #   - cron: "0 */4 * * *"    # Every 4 hours
  #   - cron: "0 8,12,17 * * *" # 8am, noon, 5pm UTC
  #   - cron: "*/30 * * * *"    # Every 30 minutes

  # â”€â”€ On push to main (for config changes) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # push:
  #   branches: [main]
  #   paths: [crawler/**]

# Required for GitHub Pages deployment
permissions:
  contents: read
  pages: write
  id-token: write

# Only allow one crawl at a time â€” cancel any in-progress run
concurrency:
  group: "pages-deploy"
  cancel-in-progress: true

env:
  # For scheduled runs, set your default URL here:
  DEFAULT_SITE_URL: "https://cabin-hem-65260868.figma.site/"

jobs:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #  Job 1: Crawl the site
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  crawl:
    name: ğŸ•·ï¸ Crawl Site
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ğŸŸ¢ Setup Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: ğŸ“¦ Install crawler dependencies
        working-directory: crawler
        run: npm ci || npm install

      - name: ğŸ­ Install Playwright Chromium
        working-directory: crawler
        run: npx playwright install chromium --with-deps

      - name: ğŸ•·ï¸ Run crawler
        working-directory: crawler
        env:
          SITE_URL: ${{ inputs.site_url || env.DEFAULT_SITE_URL }}
          OUTPUT_DIR: ./static-output
          MAX_PAGES: ${{ inputs.max_pages || '50' }}
          WAIT_FOR_NETWORK: ${{ inputs.wait_for_network || 'true' }}
          CONCURRENCY: "3"
          PAGE_TIMEOUT: "30000"
          DOWNLOAD_ASSETS: "true"
          RENDER_DELAY: "3000"
        run: node crawl.js

      - name: ğŸ“Š Build summary
        working-directory: crawler
        run: |
          MANIFEST="./static-output/manifest.json"
          echo "## ğŸ•·ï¸ Crawl Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "$MANIFEST" ]; then
            COUNT=$(jq length "$MANIFEST")
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Pages crawled | **$COUNT** |" >> $GITHUB_STEP_SUMMARY
            echo "| Source | \`${{ inputs.site_url || env.DEFAULT_SITE_URL }}\` |" >> $GITHUB_STEP_SUMMARY
            echo "| Timestamp | $(date -u '+%Y-%m-%d %H:%M UTC') |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "<details><summary>Pages</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            jq -r '.[] | "- **\(.title // "Untitled")** â†’ `\(.file)`"' "$MANIFEST" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ No manifest â€” crawl may have failed." >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: ğŸ“¤ Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: crawler/static-output

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #  Job 2: Deploy to GitHub Pages
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  deploy:
    name: ğŸš€ Deploy to GitHub Pages
    needs: crawl
    runs-on: ubuntu-latest

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: ğŸš€ Deploy
        id: deployment
        uses: actions/deploy-pages@v4
